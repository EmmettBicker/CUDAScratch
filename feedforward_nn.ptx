//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34714021
// Cuda compilation tools, release 12.6, V12.6.68
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_52
.address_size 64

	// .globl	_Z8mat_multiiPfS_S_

.visible .entry _Z8mat_multiiPfS_S_(
	.param .u32 _Z8mat_multiiPfS_S__param_0,
	.param .u32 _Z8mat_multiiPfS_S__param_1,
	.param .u64 _Z8mat_multiiPfS_S__param_2,
	.param .u64 _Z8mat_multiiPfS_S__param_3,
	.param .u64 _Z8mat_multiiPfS_S__param_4
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<27>;


	ld.param.u32 	%r11, [_Z8mat_multiiPfS_S__param_0];
	ld.param.u32 	%r12, [_Z8mat_multiiPfS_S__param_1];
	ld.param.u64 	%rd15, [_Z8mat_multiiPfS_S__param_2];
	ld.param.u64 	%rd16, [_Z8mat_multiiPfS_S__param_3];
	ld.param.u64 	%rd14, [_Z8mat_multiiPfS_S__param_4];
	cvta.to.global.u64 	%rd1, %rd15;
	cvta.to.global.u64 	%rd2, %rd16;
	mov.u32 	%r13, %ntid.y;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %tid.y;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r12;
	@%p1 bra 	$L__BB0_9;

	setp.lt.s32 	%p2, %r11, 1;
	mov.f32 	%f29, 0f00000000;
	@%p2 bra 	$L__BB0_8;

	add.s32 	%r17, %r11, -1;
	and.b32  	%r24, %r11, 3;
	setp.lt.u32 	%p3, %r17, 3;
	mov.f32 	%f29, 0f00000000;
	mov.u32 	%r23, 0;
	@%p3 bra 	$L__BB0_5;

	sub.s32 	%r22, %r11, %r24;
	mul.lo.s32 	%r19, %r11, %r1;
	mul.wide.s32 	%rd17, %r19, 4;
	add.s64 	%rd24, %rd2, %rd17;
	mov.f32 	%f29, 0f00000000;
	mov.u32 	%r23, 0;
	mov.u64 	%rd23, %rd1;

$L__BB0_4:
	ld.global.f32 	%f12, [%rd23];
	ld.global.f32 	%f13, [%rd24];
	fma.rn.f32 	%f14, %f13, %f12, %f29;
	ld.global.f32 	%f15, [%rd23+4];
	ld.global.f32 	%f16, [%rd24+4];
	fma.rn.f32 	%f17, %f16, %f15, %f14;
	ld.global.f32 	%f18, [%rd23+8];
	ld.global.f32 	%f19, [%rd24+8];
	fma.rn.f32 	%f20, %f19, %f18, %f17;
	ld.global.f32 	%f21, [%rd23+12];
	ld.global.f32 	%f22, [%rd24+12];
	fma.rn.f32 	%f29, %f22, %f21, %f20;
	add.s32 	%r23, %r23, 4;
	add.s64 	%rd24, %rd24, 16;
	add.s64 	%rd23, %rd23, 16;
	add.s32 	%r22, %r22, -4;
	setp.ne.s32 	%p4, %r22, 0;
	@%p4 bra 	$L__BB0_4;

$L__BB0_5:
	setp.eq.s32 	%p5, %r24, 0;
	@%p5 bra 	$L__BB0_8;

	mul.wide.s32 	%rd18, %r23, 4;
	add.s64 	%rd26, %rd1, %rd18;
	mad.lo.s32 	%r20, %r11, %r1, %r23;
	mul.wide.s32 	%rd19, %r20, 4;
	add.s64 	%rd25, %rd2, %rd19;

$L__BB0_7:
	.pragma "nounroll";
	ld.global.f32 	%f23, [%rd26];
	ld.global.f32 	%f24, [%rd25];
	fma.rn.f32 	%f29, %f24, %f23, %f29;
	add.s64 	%rd26, %rd26, 4;
	add.s64 	%rd25, %rd25, 4;
	add.s32 	%r24, %r24, -1;
	setp.ne.s32 	%p6, %r24, 0;
	@%p6 bra 	$L__BB0_7;

$L__BB0_8:
	cvta.to.global.u64 	%rd20, %rd14;
	mul.wide.s32 	%rd21, %r1, 4;
	add.s64 	%rd22, %rd20, %rd21;
	st.global.f32 	[%rd22], %f29;

$L__BB0_9:
	ret;

}
	// .globl	_Z3addiPfS_
.visible .entry _Z3addiPfS_(
	.param .u32 _Z3addiPfS__param_0,
	.param .u64 _Z3addiPfS__param_1,
	.param .u64 _Z3addiPfS__param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u32 	%r2, [_Z3addiPfS__param_0];
	ld.param.u64 	%rd1, [_Z3addiPfS__param_1];
	ld.param.u64 	%rd2, [_Z3addiPfS__param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.f32 	%f1, [%rd7];
	ld.global.f32 	%f2, [%rd5];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd7], %f3;

$L__BB1_2:
	ret;

}
	// .globl	_Z4reluiPf
.visible .entry _Z4reluiPf(
	.param .u32 _Z4reluiPf_param_0,
	.param .u64 _Z4reluiPf_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r2, [_Z4reluiPf_param_0];
	ld.param.u64 	%rd1, [_Z4reluiPf_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	mov.f32 	%f2, 0f00000000;
	max.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd4], %f3;

$L__BB2_2:
	ret;

}
	// .globl	_Z7softmaxiPfS_
.visible .entry _Z7softmaxiPfS_(
	.param .u32 _Z7softmaxiPfS__param_0,
	.param .u64 _Z7softmaxiPfS__param_1,
	.param .u64 _Z7softmaxiPfS__param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<128>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<24>;


	ld.param.u32 	%r20, [_Z7softmaxiPfS__param_0];
	ld.param.u64 	%rd13, [_Z7softmaxiPfS__param_1];
	ld.param.u64 	%rd14, [_Z7softmaxiPfS__param_2];
	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd14;
	mov.u32 	%r21, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r1, %r22, %r21, %r23;
	setp.ge.s32 	%p1, %r1, %r20;
	@%p1 bra 	$L__BB3_16;

	ld.global.f32 	%f122, [%rd2];
	setp.lt.s32 	%p2, %r20, 1;
	@%p2 bra 	$L__BB3_8;

	add.s32 	%r25, %r20, -1;
	and.b32  	%r45, %r20, 3;
	setp.lt.u32 	%p3, %r25, 3;
	mov.u32 	%r44, 0;
	@%p3 bra 	$L__BB3_5;

	sub.s32 	%r43, %r20, %r45;
	mov.u32 	%r44, 0;
	mov.u64 	%rd20, %rd1;

$L__BB3_4:
	ld.global.f32 	%f17, [%rd20];
	max.f32 	%f18, %f122, %f17;
	ld.global.f32 	%f19, [%rd20+4];
	max.f32 	%f20, %f18, %f19;
	ld.global.f32 	%f21, [%rd20+8];
	max.f32 	%f22, %f20, %f21;
	ld.global.f32 	%f23, [%rd20+12];
	max.f32 	%f122, %f22, %f23;
	add.s32 	%r44, %r44, 4;
	add.s64 	%rd20, %rd20, 16;
	add.s32 	%r43, %r43, -4;
	setp.ne.s32 	%p4, %r43, 0;
	@%p4 bra 	$L__BB3_4;

$L__BB3_5:
	setp.eq.s32 	%p5, %r45, 0;
	@%p5 bra 	$L__BB3_8;

	mul.wide.s32 	%rd15, %r44, 4;
	add.s64 	%rd21, %rd1, %rd15;

$L__BB3_7:
	.pragma "nounroll";
	ld.global.f32 	%f24, [%rd21];
	max.f32 	%f122, %f122, %f24;
	add.s64 	%rd21, %rd21, 4;
	add.s32 	%r45, %r45, -1;
	setp.ne.s32 	%p6, %r45, 0;
	@%p6 bra 	$L__BB3_7;

$L__BB3_8:
	mov.f32 	%f127, 0f00000000;
	@%p2 bra 	$L__BB3_15;

	add.s32 	%r28, %r20, -1;
	and.b32  	%r49, %r20, 3;
	setp.lt.u32 	%p8, %r28, 3;
	mov.f32 	%f127, 0f00000000;
	mov.u32 	%r48, 0;
	@%p8 bra 	$L__BB3_12;

	sub.s32 	%r47, %r20, %r49;
	mov.f32 	%f127, 0f00000000;
	mov.u32 	%r48, 0;
	mov.u64 	%rd22, %rd1;

$L__BB3_11:
	ld.global.f32 	%f29, [%rd22];
	sub.f32 	%f30, %f29, %f122;
	mov.f32 	%f31, 0f3F000000;
	mov.f32 	%f32, 0f3BBB989D;
	fma.rn.f32 	%f33, %f30, %f32, %f31;
	cvt.sat.f32.f32 	%f34, %f33;
	mov.f32 	%f35, 0f4B400001;
	mov.f32 	%f36, 0f437C0000;
	fma.rm.f32 	%f37, %f34, %f36, %f35;
	add.f32 	%f38, %f37, 0fCB40007F;
	neg.f32 	%f39, %f38;
	mov.f32 	%f40, 0f3FB8AA3B;
	fma.rn.f32 	%f41, %f30, %f40, %f39;
	mov.f32 	%f42, 0f32A57060;
	fma.rn.f32 	%f43, %f30, %f42, %f41;
	mov.b32 	%r30, %f37;
	shl.b32 	%r31, %r30, 23;
	mov.b32 	%f44, %r31;
	ex2.approx.ftz.f32 	%f45, %f43;
	fma.rn.f32 	%f46, %f45, %f44, %f127;
	ld.global.f32 	%f47, [%rd22+4];
	sub.f32 	%f48, %f47, %f122;
	fma.rn.f32 	%f49, %f48, %f32, %f31;
	cvt.sat.f32.f32 	%f50, %f49;
	fma.rm.f32 	%f51, %f50, %f36, %f35;
	add.f32 	%f52, %f51, 0fCB40007F;
	neg.f32 	%f53, %f52;
	fma.rn.f32 	%f54, %f48, %f40, %f53;
	fma.rn.f32 	%f55, %f48, %f42, %f54;
	mov.b32 	%r32, %f51;
	shl.b32 	%r33, %r32, 23;
	mov.b32 	%f56, %r33;
	ex2.approx.ftz.f32 	%f57, %f55;
	fma.rn.f32 	%f58, %f57, %f56, %f46;
	ld.global.f32 	%f59, [%rd22+8];
	sub.f32 	%f60, %f59, %f122;
	fma.rn.f32 	%f61, %f60, %f32, %f31;
	cvt.sat.f32.f32 	%f62, %f61;
	fma.rm.f32 	%f63, %f62, %f36, %f35;
	add.f32 	%f64, %f63, 0fCB40007F;
	neg.f32 	%f65, %f64;
	fma.rn.f32 	%f66, %f60, %f40, %f65;
	fma.rn.f32 	%f67, %f60, %f42, %f66;
	mov.b32 	%r34, %f63;
	shl.b32 	%r35, %r34, 23;
	mov.b32 	%f68, %r35;
	ex2.approx.ftz.f32 	%f69, %f67;
	fma.rn.f32 	%f70, %f69, %f68, %f58;
	ld.global.f32 	%f71, [%rd22+12];
	sub.f32 	%f72, %f71, %f122;
	fma.rn.f32 	%f73, %f72, %f32, %f31;
	cvt.sat.f32.f32 	%f74, %f73;
	fma.rm.f32 	%f75, %f74, %f36, %f35;
	add.f32 	%f76, %f75, 0fCB40007F;
	neg.f32 	%f77, %f76;
	fma.rn.f32 	%f78, %f72, %f40, %f77;
	fma.rn.f32 	%f79, %f72, %f42, %f78;
	mov.b32 	%r36, %f75;
	shl.b32 	%r37, %r36, 23;
	mov.b32 	%f80, %r37;
	ex2.approx.ftz.f32 	%f81, %f79;
	fma.rn.f32 	%f127, %f81, %f80, %f70;
	add.s32 	%r48, %r48, 4;
	add.s64 	%rd22, %rd22, 16;
	add.s32 	%r47, %r47, -4;
	setp.ne.s32 	%p9, %r47, 0;
	@%p9 bra 	$L__BB3_11;

$L__BB3_12:
	setp.eq.s32 	%p10, %r49, 0;
	@%p10 bra 	$L__BB3_15;

	mul.wide.s32 	%rd16, %r48, 4;
	add.s64 	%rd23, %rd1, %rd16;

$L__BB3_14:
	.pragma "nounroll";
	ld.global.f32 	%f82, [%rd23];
	sub.f32 	%f83, %f82, %f122;
	mov.f32 	%f84, 0f3F000000;
	mov.f32 	%f85, 0f3BBB989D;
	fma.rn.f32 	%f86, %f83, %f85, %f84;
	cvt.sat.f32.f32 	%f87, %f86;
	mov.f32 	%f88, 0f4B400001;
	mov.f32 	%f89, 0f437C0000;
	fma.rm.f32 	%f90, %f87, %f89, %f88;
	add.f32 	%f91, %f90, 0fCB40007F;
	neg.f32 	%f92, %f91;
	mov.f32 	%f93, 0f3FB8AA3B;
	fma.rn.f32 	%f94, %f83, %f93, %f92;
	mov.f32 	%f95, 0f32A57060;
	fma.rn.f32 	%f96, %f83, %f95, %f94;
	mov.b32 	%r38, %f90;
	shl.b32 	%r39, %r38, 23;
	mov.b32 	%f97, %r39;
	ex2.approx.ftz.f32 	%f98, %f96;
	fma.rn.f32 	%f127, %f98, %f97, %f127;
	add.s64 	%rd23, %rd23, 4;
	add.s32 	%r49, %r49, -1;
	setp.ne.s32 	%p11, %r49, 0;
	@%p11 bra 	$L__BB3_14;

$L__BB3_15:
	mul.wide.s32 	%rd17, %r1, 4;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.f32 	%f99, [%rd18];
	sub.f32 	%f100, %f99, %f122;
	mov.f32 	%f101, 0f3F000000;
	mov.f32 	%f102, 0f3BBB989D;
	fma.rn.f32 	%f103, %f100, %f102, %f101;
	cvt.sat.f32.f32 	%f104, %f103;
	mov.f32 	%f105, 0f4B400001;
	mov.f32 	%f106, 0f437C0000;
	fma.rm.f32 	%f107, %f104, %f106, %f105;
	add.f32 	%f108, %f107, 0fCB40007F;
	neg.f32 	%f109, %f108;
	mov.f32 	%f110, 0f3FB8AA3B;
	fma.rn.f32 	%f111, %f100, %f110, %f109;
	mov.f32 	%f112, 0f32A57060;
	fma.rn.f32 	%f113, %f100, %f112, %f111;
	mov.b32 	%r40, %f107;
	shl.b32 	%r41, %r40, 23;
	mov.b32 	%f114, %r41;
	ex2.approx.ftz.f32 	%f115, %f113;
	mul.f32 	%f116, %f115, %f114;
	div.rn.f32 	%f117, %f116, %f127;
	add.s64 	%rd19, %rd2, %rd17;
	st.global.f32 	[%rd19], %f117;

$L__BB3_16:
	ret;

}

